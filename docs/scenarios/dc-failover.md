# Сценарий: DC Failover (EU недоступен)

[← Назад к README](../../README.md) | [Открыть визуализатор](https://iltsarev.github.io/bigtechDesign25/)

## Обзор

Этот сценарий демонстрирует автоматическое переключение трафика при недоступности целого датацентра. EU DC перестаёт отвечать, и Global Load Balancer перенаправляет запросы в US DC — прозрачно для пользователя.

## Последовательность событий

### 1. Нормальный путь (до сбоя)

```
Client → DNS → CDN → Global LB → EU DC (предпочтительный)
```

Global LB выбирает EU DC как ближайший к европейскому клиенту.

### 2. Обнаружение сбоя

```
Global LB ← Health Check FAILED ← EU DC
```

**Health Check параметры:**
- Интервал: 10 секунд
- Timeout: 5 секунд
- Unhealthy threshold: 3 последовательных сбоя
- Итого время обнаружения: ~30 секунд

**Что проверяется:**
- HTTP 200 на `/health` endpoint
- Latency < 1000ms
- Доступность критических зависимостей (DB, Redis)

### 3. Переключение трафика

```
Global LB → US DC (следующий по приоритету)
```

**Решение Global LB:**
1. EU DC помечен как unhealthy
2. Проверка доступных альтернатив: US DC, Asia DC
3. Выбор US DC: ниже latency для европейского клиента (100ms vs 200ms до Asia)
4. Все новые запросы направляются в US DC

### 4. Прозрачность для клиента

Клиент не знает о failover:
- DNS не меняется (Anycast IP тот же)
- CDN прозрачно перенаправляет
- Latency увеличивается (~100ms → ~200ms), но сервис работает

## Типы отказов

### Полный отказ ДЦ
- Сетевой разрыв (network partition)
- Отказ питания
- Природные катастрофы

**Решение:** Переключение на другой ДЦ

### Частичный отказ
- Перегрузка отдельных сервисов
- Сбой базы данных

**Решение:** Circuit Breaker + partial failover

### Деградация производительности
- Высокий latency из-за нагрузки
- Потеря пакетов в сети

**Решение:** Weighted routing, постепенное переключение

## Паттерны отказоустойчивости

### Active-Active
Все датацентры обслуживают трафик одновременно.

**Преимущества:**
- Эффективное использование ресурсов
- Низкий latency для всех регионов
- Мгновенное переключение

**Сложности:**
- Репликация данных между ДЦ
- Конфликты при записи
- Сложность отладки

### Active-Passive
Резервный ДЦ включается только при сбое primary.

**Преимущества:**
- Простота архитектуры
- Нет проблем с консистентностью

**Недостатки:**
- Ресурсы простаивают
- Холодный старт при failover
- Выше время переключения

## Консистентность данных

### Проблема Split-Brain

При network partition оба ДЦ могут считать себя primary:

```
[EU DC] ←✕ network partition ✕→ [US DC]
    ↓                              ↓
  writes                        writes
    ↓                              ↓
  conflicts!                   conflicts!
```

### Решения

1. **Quorum-based writes** — запись успешна при подтверждении от большинства
2. **Leader election** — только один ДЦ принимает writes
3. **CRDT** — структуры данных без конфликтов
4. **Last-Write-Wins** — простое разрешение по timestamp

## Метрики

| Метрика | SLO | Описание |
|---------|-----|----------|
| RTO (Recovery Time Objective) | < 1 min | Время восстановления сервиса |
| RPO (Recovery Point Objective) | < 10 sec | Максимальная потеря данных |
| Failover Success Rate | > 99.9% | Успешность автопереключения |

## Тестирование

### Chaos Engineering

Регулярное тестирование отказоустойчивости:

```bash
# Chaos Monkey - случайное отключение инстансов
chaos-monkey --target=eu-dc --percentage=10

# GameDay - плановое отключение ДЦ
gameday --scenario=dc-failure --region=eu-central-1
```

### Drill Schedule

- **Weekly:** Отключение отдельных инстансов
- **Monthly:** Отключение availability zone
- **Quarterly:** Полный DC failover drill

## Связанные сценарии

- [Создание заказа](create-order.md) — нормальный путь запроса
- [Service Overload](service-overload.md) — защита от перегрузки
